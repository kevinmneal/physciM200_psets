---
title: "Variable selection for species distribution modeling, and diving into the MaxEnt black box"
author: "Kevin Neal"
date: "June 2, 2015"
output: 
  html_document:
    fig_caption: yes
    fig_width: 8
---

```{r init, echo=F, include=F}
require(vioplot) || {install.packages("vioplot"); library(vioplot)}
require(dplyr)
require(tidyr)
require(reshape2)
require(minerva)
require(infotheo)
require(corrplot)
require(glmnet)

setwd("C:/Users/Kevin/Google Drive/UCLA Courses or Lab meetings etc/PhysciM200/physciM200_psets")

par(mfrow=c(1,1))
old.par <- par(mar = c(0, 0, 0, 0))
par(old.par)
```

## What is Species Distribution Modeling?
- statistically predict species distribution based on known occurrences and environmental data
- useful for:
    - predicting new localities
    - understanding climatic niche
    - predicting past and future distributions
    
![Species Distribution Modeling](http://i.imgur.com/EHO6XOf.png?1)

## Doing SDM in MaxEnt
- MaxEnt: Maximum entropy
- Generally accepted as the best model for presence-only datasets
- 3 inputs: occurrence points, background points, environmental layers
  - Can also use "pseudoabsence" points instead of background points
- program examines environmental values at occurrence points and background points
- outputs: prediction map, stats on environmental variable importance and model fit

![Species Distribution Modeling](http://i.imgur.com/EHO6XOf.png?1)

## Utility of SDMs:
- understand environmental contributions to range limits
- predict range shift under different climate scenarios
- estimate past range to explore ancient hybridization with currently-allopatric congeners

## spadefoot toad
- Spea hammondii - western spadefoot toad
![Spea hammondii](http://i.imgur.com/PsRfF6F.jpg?1)

## spadefoot toad
- *spadefoot*
!["spadefoot"](http://i.imgur.com/Au6eyjP.jpg?1)


## Species occurrence points
- GBIF (Global Biodiversity Information Facility)
- raw GBIF data: 940 occurrences
- use spatial grid subsampling to limit effect of sampling bias in the model (are number of samples at a given locality a real indicator of habitat suitability, or an artifact?)

## Environmental layers
- Bioclim variables: biologically-relevant measures of climate
    - Bio1 to Bio19
    - e.g. Annual mean temperature (bio1), min temp. of coldest month (bio6), annual precip (bio12), precip of warmest quarter (bio18), etc.
## Past and present environmental layers
- can project model fit to current climate data to other climate layers

## Bioclim layers
- each raster contains a range of values over the map area used in the model
- imagine all stacked and taking value at a point on top of all - that occurrence point corresponds to a value in all 19 of these layers
![climate variable values](http://i.imgur.com/2XnNPev.png?1)

## Running the MaxEnt model
- In addition to prediction maps, can use other methods to test model fit and significance
- Maxent already implements bootstrapping, for instance, but running it takes awhile
    - Cross-validation (divide occurrence points into test and training data)
    - subsampling without replacement
    - bootstrapping (with replacement)
    
## Jackknifing
- iteratively omit each variable and re-run model to test environmental variable importance
- can see bio18 (precip of warmest quarter) by itself is the best predictor variable in the model, when I ran it with all 19 variables
![jackknifing](http://i.imgur.com/o4jH6lo.jpg?1)

## prediction map
![present distribution](http://i.imgur.com/HILCUgcl.png?1)

## variable contributions
- bio18: precip of warmest quarter
- bio19: precip of coldest quarter
- bio14: precip of driest month
- bio15: precip seasonality (coeff. of variation)
![variable contributions](http://i.imgur.com/bEe4FDc.png?1)

## Run model for other scenarios
![all models](http://i.imgur.com/ec6Ikpp.png?1)

## Hard to really improve on the existing Maxent model...
- Many publications comparing with other methods; always superior.
- Goals here, then:
  - Generate a procedure for removing highly correlated variables
  - demystify the "black box" that is Maxent!

# Collinearity in environmental variables for species distribution modeling

## To avoid overfitting, want to remove collinear variables before throwing them into the black box
- overfitting to present conditions could also impact projections on past and future climate scenarios

```{r loading points, echo=F}
# using spea and bioclim variables
spea.ll <- read.csv("speaLatLonBioclim.csv")
spea.ll <- spea.ll[-1]
spea <- spea.ll[-c(1:2)] # creates spea.bc which is ONLY bioclim variables, no latlon

colnames(spea.ll)[1] <- "lon" # simplifying column names
colnames(spea.ll)[2] <- "lat"
spea2 <- spea

present <- rep(1, nrow(spea))
spea2[,20] <- present
colnames(spea2)[20] <- "pres"
#summary(spea)
#corrplot(cor(spea, method="spearman"))
#pairs(spea)

### "absence" points (sampled from random background points on same grid as presence points, using chess="black" while presence points sampled from chess="white")

absent.ll <- read.csv("speaabsentLatLonBioclim.csv")
absent.ll <- absent.ll[-1]
absent <- absent.ll[,-c(1:2)] # creates dataframe of ONLY bioclim variables, no latlon  
colnames(absent.ll)[1] <- "lon" # simplifying column names
colnames(absent.ll)[2] <- "lat"
absent2 <- absent

absences <- rep(0, nrow(absent))
absent2[,20] <- absences
colnames(absent2)[20] <- "pres"

allpts <- rbind(spea2, absent2)
hammy <- allpts # hammy refers to hammondii. This is the dataframe with all the presence AND absence points AND bioclim values. No latlon.
hammy.ll <- cbind(hammy, rbind(spea.ll[,c(1:2)], absent.ll[,c(1:2)])) # the dataframe INCLUDING latlon. Probably won't need for now.

```


### Stepwise regression
```{r using GLM}
#options(show.signif.stars = T) # cuz why not
#names(spea)
#model1 <- lm(bio6 ~ bio1 + bio2 + bio3 + bio4 + bio5, data=spea)
#summary(model1) # murder and population significant; area and illiteracy worst predictors. Tutorial removes area...
#mine(spea)

# With GLM: specify binomial family

family <- binomial(link = "logit")


# GLM with presence-only:
#spea.glm <- glm(pres ~ bio1+bio2+bio3+bio4+bio5+bio6+bio7+bio8+bio9+bio10+bio11+bio12+bio13+bio14+bio15+bio16+bio17+bio18+bio19, family=binomial(link = "logit"), data=spea2)
#summary(spea.glm)
#step(spea.glm, direction="backward") # this fails without absence points. Kind of expected...  
# step() on presence-only gives best-fit as: pres ~ bio1 + bio2 + bio3 + bio4 + bio5 + bio6 + bio8 + bio9 + bio11 + bio12 + bio14 + bio15 + bio16 + bio18

# GLM with presence and absence:
hammy.glm <- glm(pres ~ bio1+bio2+bio3+bio4+bio5+bio6+bio7+bio8+bio9+bio10+bio11+bio12+bio13+bio14+bio15+bio16+bio17+bio18+bio19, family=binomial(link = "logit"), data=hammy)
summary(hammy.glm)
step(hammy.glm, direction="backward")

# step() using the logistic glm with OLDER data gives best-fit as: pres ~  bio8 + bio9 + bio10 + bio11 + bio14 + bio15 + bio18 + bio19
# newer one with fewer absences: bio1 + bio4 + bio9 + bio14 + bio15 + bio17 + bio18 + bio19
plot(hammy.glm)

plot(pres~bio8+bio9, data=hammy)
# anova(spea.glm, spea.lm)
# confint()
# predict(hammy.glm, type="response") # confusing...

```







### Bootstrapping
```{r bootstrapping, eval=F}
### For this with Spea and presence/"absence" data, could shuffle ONLY the presence/absence column ... which is the NHST below I guess


# bootstrap spearman correlations using replicate()
draws <- replicate(1000, {
  i <- sample(nrow(spea), replace=TRUE)
  data <- spea[i,]
  cor(data, method="spearman")
})

#str(draws) 
#colnames(draws)
#rownames(draws)
#draws[,,2]
#median(draws[2,3,])
#colnames(draws)[2]

actual.cor <- cor(spea)
par(mfrow=c(19,19))
#par("mar") # checks current margin dimensions
par(mar=c(1.5,1,1,1), cex.main=0.8, cex.axis=0.8, cex.lab=0.8, mgp=c(1,0,0))
# using mfcol plots down instead of across
# reset with layout(matrix(1))

for(i in 1:19){
  for(j in 1:19){
    if(i==j){
      plot.new()
    } else {
    hist(draws[i,j,], main=paste(colnames(draws)[i], "~", colnames(draws)[j]), xlab="")
    abline(v = quantile(draws[i,j,], c(0.025, 0.975)), col="blue") # blue lines for 95CI
    abline(v = actual.cor[i,j], col="red") # red line for actual median
    print(paste(colnames(draws)[i], "~", colnames(draws)[j], ": ", round(median(draws[i,j,]), digits=4), " (", round(sort(draws[i,j,])[0.025*1000], digits=4), ", ", round(sort(draws[i,j,])[0.975*1000], digits=4), ")", sep=""))
    }
    
  }
}

```

Red lines are observed Spearman correlation coefficients. Blue lines are 95% confidence intervals based on 10,000 bootstraps.

### Null Hypothesis Significance Testing
```{r NHST, eval=F}
#cor(spea)

draws.nh <- replicate(1000, {
  data <- apply(spea, 2, sample) # shuffles data within columns for doing NHST
  cor(data, method="spearman")
})

# prints one-sided p-value, using absolute value to account for negative correlations
speabc.pvals <- data.frame(matrix(NA, nrow=19, ncol=19))
row.names(speabc.pvals) <- (colnames(draws.nh))
#str(sep.pvals)
#sep.pvals[] <- NA
par(mfrow=c(19,19))
#par("mar") # checks current margin dimensions
par(mar=c(1.5,1,1,1), cex.main=0.8, cex.axis=0.8, cex.lab=0.8, mgp=c(1,0,0))

for(i in 1:19){
  for(j in 1:19){
    if(i==j){
      plot.new()
    } else {
    hist(draws.nh[i,j,], main=paste(colnames(draws.nh)[i], "~", colnames(draws.nh)[j]), xlab="")
    abline(v = quantile(draws.nh[i,j,], c(0.025, 0.975)), col="blue") # blue lines for 95CI
    abline(v = actual.cor[i,j], col="red") # red line for actual median
    speabc.pvals[i,j] <- length(draws.nh[i,j,draws.nh[i,j,]>abs(actual.cor[i,j])])/1000
    #print(paste(colnames(draws.nh)[i], ", ", colnames(draws.nh)[j], (length(draws.nh[i,j,draws.nh[i,j,]>abs(actual.cor[i,j])])/10000)))
    
    }
  }
}
par(mfrow=c(1,1))
colnames(speabc.pvals) <- colnames(draws.nh)
rownames(speabc.pvals) <- colnames(draws.nh)


```
Null hypothesis significance testing (null: Spearman correlation is zero). Red lines are observed Spearman correlation coefficients. Blue lines are 95% confidence intervals based on 1000 bootstraps.  


```{r}

hist(spea2[,1], col="red")
par(mfrow=c(1,2))
hist(absent2[,1], col="blue")


```

```{r MIC}
# MIC tablecolumns:
# MIC >= x,gives p <= y,+/- (with 95% confidence)

# all original predictors
#sepMIC <- mine(sep)$MIC
#micpval <- read.csv("n=35,alpha=0.6.csv", header=T, na.strings = c('n','0','bad','?','NA','---','-', ' ')) # downloaded p-values for n=35
#par(mfrow=c(1,2))
#corrplot(mine(sep)$MIC, main="sepsis MIC")
#corrplot(cor(sep, method="spearman", use="pairwise.complete.obs"), main="spearman")

#par(mfrow=c(1,1))

# 4 significant predictors
spea.bioclimMIC <- mine(hammy)$MIC
micpval <- read.csv("n=55,alpha=0.6.csv", header=T, na.strings = c('n','0','bad','?','NA','---','-', ' ')) # downloaded p-values for n=35
par(mfrow=c(1,2))
corrplot(mine(hammy)$MIC, main="spea MIC")
corrplot(cor(hammy, method="spearman", use="pairwise.complete.obs"), main="spearman")


micpval[(micpval$MIC<(spea.bioclimMIC[2,1]+0.00001) & micpval$MIC>(spea.bioclimMIC[2,1]-0.00001)),]
# min(micpval$MIC) # MIC values in table range from 0.4328 to 0.9207. Anything above 0.9207 has pval = 0 more or less; MIC less than 0.4238 is > 0.05
mic.pvals <- data.frame(matrix(NA, nrow=19, ncol=19))

#str(mic.pvals)

for(i in 1:19){
  for(j in 1:19){ 
    mic.pvals[i,j] <- micpval[which.min(abs(micpval$MIC - spea.bioclimMIC[i,j])),][2]
  }
}
    
colnames(mic.pvals) <- colnames(spea)
rownames(mic.pvals) <- colnames(spea)

mic.pvals2 <- mic.pvals
#mic.pvals2[mic.pvals2>0.05] <- "NS"
#mic.pvals
spea.bioclimMIC.high <- spea.bioclimMIC
#spea.bioclimMIC.high[spea.bioclimMIC.high>=0.7] <- "HIGH"

#write.csv(mic.pvals, file="speaBioclim_MICpvals.csv")
```




Add the p-value matrix with the MIC value matrix... find cells that are high and significant:

```{r}

sp.MIC.high2 <- spea.bioclimMIC.high
sp.MIC.high2[sp.MIC.high2>=0.9] <- 800.0 # any high values now equal 800 to make addition with p-value matrix more obvious... and making significant p-values equal 200. So any high and significant cells will equal 1000 when added. Probably a much easier way to do this but whatever

mic.pvals.toadd <- mic.pvals
mic.pvals.toadd[mic.pvals.toadd<0.005] <- 200.0

sp.MIC.highandsig <- sp.MIC.high2 + mic.pvals.toadd


#as.matrix(sp.MIC.high2) + as.matrix(sp.MIC.high2)

#sp.MIC.highandsig <- apply(sp.MIC.high2, 2, FUN=as.numeric) + apply(mic.pvals.toadd, 2, FUN=as.numeric) use when non-numeric
sp.MIC.highandsig.df <- as.data.frame(sp.MIC.highandsig)

```


```{r, eval=F}
Significant MIC correlations (>0.7, arbitrarily...) with p < 0.005 (arbitrary cutoff... but I can't take everything out...):

bio3-bio4
bio3-bio7
bio3-bio9
bio3-bio10
bio4-bio5
bio4-bio7
bio4-bio9
bio4-bio10
bio5-bio9
bio5-bio10
bio6-bio8
bio6-bio10
bio6-bio11
bio7-bio9
bio8-bio11
bio9-bio10
bio12-bio13
bio12-bio16
bio12-bio19
bio13-bio16
bio13-bio19
bio16-bio19

Significant MIC correlations (>0.9, arbitrarily...) with p < 0.005 (arbitrary cutoff... but I can't take everything out...):

bio3-bio4
bio4-bio7
bio5-bio7
bio9-bio10
bio12-bio13
bio12-bio16
bio12-bio19
bio13-bio16
bio13-bio19
bio16-bio19

number of pairs each biolayer is present in:
bio3: 1
bio4: 2
bio5: 1
bio7: 2
bio9: 1
bio10: 1
bio12: 4
bio13: 4
bio16: 3
bio19: 3
```

Sum columns - those with MOST mutual information should be removed? Maybe?

```{r}



#sum(spea.bioclimMIC[,12])
#sum(spea.bioclimMIC[,13]) # so 12 has more total mutual info than 13... 
#sum(spea.bioclimMIC[,1])

speaMICsum <- rep(NA, 19)
for(i in 1:19){
  speaMICsum[i] <- sum(spea.bioclimMIC[,i])
}
speaMICsum # is this informative though? here, bio4 has the most mutual information... should it be removed, then? does bio14 have the most unique information because it has the lowest total MIC?

```

```{r, echo=F, eval=F, include=F}



Should I use "presence" as the dependent variable? Do I just score it as 0 = absent and 1 = present and include some pseudoabsence points?
I could use Maxent values of probability, but that would be circular as fuck. Maybe a priori downweight pseudoabsence points by distance from a presence point so that they aren't all equal to zero...?  


Try this out with just presences... so add a column to the original input with just 1's
```

```{r}
spea2 <- spea

present <- rep(0:1, 27)
spea2[,20] <- present
colnames(spea2)[20] <- "present"

par(mfrow=c(1,2))
corrplot(mine(spea2)$MIC, main="spea MIC")
corrplot(cor(spea2, method="spearman", use="pairwise.complete.obs"), main="spearman")


# BIO1 = Annual Mean Temperature
# BIO2 = Mean Diurnal Range (Mean of monthly (max temp - min temp))
# BIO3 = Isothermality (BIO2/BIO7) (* 100)
# BIO4 = Temperature Seasonality (standard deviation *100)
# BIO5 = Max Temperature of Warmest Month
# BIO6 = Min Temperature of Coldest Month
# BIO7 = Temperature Annual Range (BIO5-BIO6)
# BIO8 = Mean Temperature of Wettest Quarter
# BIO9 = Mean Temperature of Driest Quarter
# BIO10 = Mean Temperature of Warmest Quarter
# BIO11 = Mean Temperature of Coldest Quarter
# BIO12 = Annual Precipitation
# BIO13 = Precipitation of Wettest Month
# BIO14 = Precipitation of Driest Month
# BIO15 = Precipitation Seasonality (Coefficient of Variation)
# BIO16 = Precipitation of Wettest Quarter
# BIO17 = Precipitation of Driest Quarter
# BIO18 = Precipitation of Warmest Quarter
# BIO19 = Precipitation of Coldest Quarter

## 12, 13, 16, 19 highly correlated! Because almost all of CA's precipitation falls in the wettest month/quarter, which also tends to be the coldest month/quarter

?minerva
```

## Things to explore further:  
- using real binary data (1=present, 0=absent; I just randomly made some points absences here for testrun) and methods to deal this binary dependent variable (LASSO? dead/alive)  
- variable selection with MIC  
- other machine learning methods (random forests?)  
- explore other "dimensionality reduction" methods?
- Does removing correlated variables really affect predictions significantly?
- Current predictions from MaxEnt already fairly reliable, but overparameterizing could have substantial effects for projections with novel climate scenarios (i.e. combinations of climate variable values not seen in present day) -- this is where the removal of collinear variables seems it would be most useful given the uncertainty of past and present climate models

Things to remove: one of 3 and 4 (probably 4); one of 9 and 10 (probably 10); one of 8 and 11 (probably 11); two or three of 12, 13, 16, and 19 (probably 13 and 19). Then with what's left, maybe keep ONLY "Quarter" and remove its "Month" equivalent? Not 5 and 6 though...


### Latest update! June 9, 11pm
Use this paper's method to use mutinformation to select the best model!: http://research.ics.aalto.fi/eiml/Publications/Publication136.pdf

by iteratively adding variables to the function. Goal is to maximize the information, with X as the variables and Y as the dependent variable (i.e. presence/absence). X is the bioclim variables. You start with running mutinformation() on the whole matrix and see which single variable has the highest mutual information value with the presence variable. This is your first variable selected. Subsequently, you will look for the greatest information addition adding a single other variable. To do this you will use cbind to put two variables in the X argument in mutinformation, and do this across all remaining variables to find the highest combination of information with your first selected variable. Like so:
```{r}
require(infotheo)

mutinformation(discretize(hammy)) # bio1 has highest mutual info
?mutinformation
hammybc <- hammy[,-20]
#mutinformation(discretize(hammybc), discretize(hammy[,20])) # MI = 0.631 including all variables

mutinformation(cbind(discretize(hammy[,1]), discretize(hammy[,2])), discretize(hammy[,20], nbins=2)) # if here for example we had chosen bio2 as our X1, we add each other bioclim to it to test the information gained. The variable that leads to the highest gain is X2. Then do the same to find X3, and so on. Do this until the information goes DOWN!!! If it never goes down, then the model with all 19 variables is the best...
# this example is still using dummy-absences, though. Get "true"" pseudoabsences AND add more presences by doing smarter spatial sampling... though this is a whole can of worms... 

# I can apply bootstraps to every step here... hmm then get confidence intervals on the MI of every added variable? One with highest median moves on?

mutin2 <- rep(NA, 19)
for (i in 1:19){
  mutin2[i] <- mutinformation(cbind(discretize(hammy[,1]), discretize(hammy[,i])), discretize(hammy[,20], nbins=2))
}

mutin2 # bio12 increases the information the most, so bio12 becomes X2.


mutin3 <- matrix(nrow=100, ncol=19)
#mutin3 <- rep(NA, 100)

for(j in 1:1000) {
  hamboot <- rbind(unique(hammy[sample(nrow(hammy[hammy$pres==1,]), 88, replace=T),]), unique(hammy[sample(nrow(hammy[hammy$pres==0,]), 176, replace=T),])) # use unique to avoid weighting certain localities higher than others
  mutin <- rep(NA, 19)
  for(i in 1:19){
    mutin[i] <- mutinformation(cbind(discretize(hamboot[,1]), discretize(hamboot[,12]), discretize(hamboot[,i])), discretize(hamboot[,20]))
  }
  mutin3[j,] <- mutin
}

# do duplicates affect MI?
hamboottest <- rbind(hammy[sample(nrow(hammy[hammy$pres==1,]), 88, replace=T),], hammy[sample(nrow(hammy[hammy$pres==0,]), 176, replace=T),])
test.383<- mutinformation(discretize(hamboottest))
test.384<- mutinformation(discretize(unique(hamboottest)))
test.383[,20]
test.384[,20]
mutinformation(discretize(test.383), discretize(test.384))

mutin3 <- replicate(100, {
  hamboot <- rbind(unique(hammy[sample(nrow(hammy[hammy$pres==1,]), 88, replace=T),]), unique(hammy[sample(nrow(hammy[hammy$pres==0,]), 176, replace=T),]))
  mutin <- rep(NA, 19)
  for(i in 1:19){
    mutin[i] <- mutinformation(cbind(discretize(hamboot[,1]), discretize(hamboot[,12]), discretize(hamboot[,i])), discretize(hamboot[,20]))
  }
  j <- 0
  j <- j + 1
  mutin3[j,] <- mutin
})


medianCI <- function(data){
  medci <- cbind(median(data), 
        quantile(data, 0.025),
        quantile(data, 0.975))
  colnames(medci) <- c("median", "low", "upp")
  return(medci)
}

mutin3CI2 <- apply(mutin3, 1, medianCI)
mutin3CI2
mutin3CI
rank(mutin3CI[1,])
rank(mutin3CI2[1,])
mutin3med <- apply(mutin3, 2, median)
mutin3med

mutin3 # in this example, bio12 adds the most information. Notice bio1 and bio2, which are already included as variables, do not increase the information if added a second time because that information is already there, and the algorithm is smart enough to realize.
# then we would move on to 4 variables, 5 variables, etc etc...
# So if you want to make this rigorous, can bootstrap (maybe without replacement because of the spatial autocorrelation stuff...) the mutual information by running 1000 permutations with random subsets of the localities.

# I think the biggest challenge here will be the pseudoabsence selection. Will have to select outside Spea's range to better reflect them as true absences. So do like, 500 random background points on California and surrounding, and then mask out anything in close proximity to true presences.  

# After using mutinformation to select variables, can test against step() with lm and glm? and other methods? And then run an actual maxent run and compare the results/compare to a full run with 19 variables???  

```

Other things I could bootstrap: the actual occurrence points. Wait, didn't I already say I would do that? Yes... but Maxent itself can do that... so idk

For absence points I'm going to randomly sample ~ 250 points from the background raster, and then clip out anything that falls within a ~10 km radius of presence points... is this biasing it? idk. Maybe use an even smaller radius... so I won't show the full procedure for that here, and will just import the csv file I make from it.